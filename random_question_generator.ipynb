{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Show full text in columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reset options to default if needed\n",
    "# pd.reset_option('display.max_rows')\n",
    "# pd.reset_option('display.max_columns')\n",
    "# pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the CUAD JSON data\n",
    "file_path = 'input_data/CUAD_v1.json'  # Update with your actual file path\n",
    "with open(file_path, 'r') as f:\n",
    "    cuad_data = json.load(f)\n",
    "\n",
    "# List to hold the combined data\n",
    "combined_data = []\n",
    "\n",
    "# Iterate through the CUAD data and combine everything into a single list\n",
    "for doc_idx, entry in enumerate(cuad_data['data']):\n",
    "    # Extract Document Info\n",
    "    document_id = f\"doc_{doc_idx+1}\"\n",
    "    doc_title = entry.get('title', 'N/A')\n",
    "    \n",
    "    for para_idx, paragraph in enumerate(entry['paragraphs']):\n",
    "        paragraph_id = f\"{document_id}_para_{para_idx+1}\"\n",
    "        paragraph_text = paragraph['context']\n",
    "        \n",
    "        for qa_idx, qa in enumerate(paragraph['qas']):\n",
    "            question_id = f\"{paragraph_id}_q_{qa_idx+1}\"\n",
    "            question_text = qa['question']\n",
    "            is_impossible = qa['is_impossible']\n",
    "            \n",
    "            # Handle answers\n",
    "            if qa['answers']:\n",
    "                for ans_idx, answer in enumerate(qa['answers']):\n",
    "                    answer_id = f\"{question_id}_a_{ans_idx+1}\"\n",
    "                    answer_text = answer['text']\n",
    "                    answer_start = answer['answer_start']\n",
    "                    \n",
    "                    # Combine all relevant data into one row\n",
    "                    combined_data.append({\n",
    "                        'document_id': document_id,\n",
    "                        'document_title': doc_title,\n",
    "                        'paragraph_id': paragraph_id,\n",
    "                        'paragraph_text': paragraph_text,\n",
    "                        'question_id': question_id,\n",
    "                        'question_text': question_text,\n",
    "                        'is_impossible': is_impossible,\n",
    "                        'answer_id': answer_id,\n",
    "                        'answer_text': answer_text,\n",
    "                        'answer_start': answer_start\n",
    "                    })\n",
    "            else:\n",
    "                # If no answers are present, still include the question\n",
    "                combined_data.append({\n",
    "                    'document_id': document_id,\n",
    "                    'document_title': doc_title,\n",
    "                    'paragraph_id': paragraph_id,\n",
    "                    'paragraph_text': paragraph_text,\n",
    "                    'question_id': question_id,\n",
    "                    'question_text': question_text,\n",
    "                    'is_impossible': is_impossible,\n",
    "                    'answer_id': None,\n",
    "                    'answer_text': None,\n",
    "                    'answer_start': None\n",
    "                })\n",
    "\n",
    "# Convert the combined data into a Pandas DataFrame\n",
    "combined_df = pd.DataFrame(combined_data)\n",
    "\n",
    "# # Save to a CSV file\n",
    "# combined_df.to_csv('cuad_combined.csv', index=False)\n",
    "\n",
    "# Optionally, print the head of the DataFrame to verify\n",
    "# print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtering data by document name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Folder path\n",
    "folder_path = 'topic_embeddings'\n",
    "\n",
    "# List all CSV files and remove everything after the first dot (.) including any extensions\n",
    "csv_files = ['.'.join(file.split('.')[:-2]) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Print the modified list of CSV files\n",
    "print(csv_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating document list available to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Folder path\n",
    "folder_path = 'topic_embeddings'\n",
    "\n",
    "# List all CSV files and remove everything after the first dot (.) including any extensions\n",
    "document_list = ['.'.join(file.split('.')[:-1]) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Print the modified list of CSV files\n",
    "print(document_list)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "document_df = pd.DataFrame(document_list, columns=['file_name'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_file = 'document_list_available_to_query.csv'\n",
    "document_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter DataFrame by a list of document titles\n",
    "def filter_by_document_titles(df, document_titles):\n",
    "    # Filter the DataFrame based on the list of document titles\n",
    "    return df[df['document_title'].isin(document_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example list of document titles to filter by\n",
    "document_titles_to_filter = csv_files\n",
    "#document_titles_to_filter =['AIRSPANNETWORKSINC_04_11_2000-EX-10.5-Distributor Agreement','BEYONDCOMCORP_08_03_2000-EX-10.2-CO-HOSTING AGREEMENT','Columbia Laboratories, (Bermuda) Ltd. - AMEND NO. 2 TO MANUFACTURING AND SUPPLY AGREEMENT.PDF','ENERGYXXILTD_05_08_2015-EX-10.13-Transportation AGREEMENT.PDF','MERCATAINC_03_09_2000-EX-10.21-SPONSORSHIP AGREEMENT.PDF','TALLGRASSENERGY,LP_02_20_2020-EX-99.26-JOINT FILING AGREEMENT.PDF','UNITEDNATIONALBANCORP_03_03_1999-EX-99-Outsourcing Agreement with the BISYS Group, Inc..PDF','VERTICALNETINC_04_01_2002-EX-10.19-MAINTENANCE AND SUPPORT AGREEMENT.PDF']\n",
    "#document_titles_to_filter=['ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT','DeltathreeInc_19991102_S-1A_EX-10.19_6227850_EX-10.19_Co-Branding Agreement_ Service Agreement']\n",
    "\n",
    "# Apply filtering based on the given document titles\n",
    "combined_df = filter_by_document_titles(combined_df, document_titles_to_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating random sample questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df[combined_df['is_impossible'] == False]\n",
    "\n",
    "\n",
    "full_sample = combined_df[['document_title', 'question_text', 'answer_text']]\n",
    "# Randomly select 15 rows\n",
    "random_sample = combined_df[['document_title', 'question_text', 'answer_text']].sample(10,random_state=5)\n",
    "\n",
    "# Save the random sample to a new DataFrame\n",
    "random_df = pd.DataFrame(random_sample)\n",
    "full_df = pd.DataFrame(full_sample)\n",
    "# Save the DataFrame to a CSV file\n",
    "random_df.to_csv('random_questions.csv', index=False)\n",
    "full_df.to_csv('full_questions.csv', index=False)\n",
    "\n",
    "print(\"Random sample of questions saved to 'random_questions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Save the final processed text as a pickle file\n",
    "pickle_file_path = 'random_question_dataframe.pkl'\n",
    "with open(pickle_file_path, 'wb') as file:\n",
    "    pickle.dump(random_df, file)\n",
    "\n",
    "print(f\"Processed text saved to {pickle_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the final processed text as a pickle file\n",
    "pickle_file_path = 'full_question_dataframe.pkl'\n",
    "with open(pickle_file_path, 'wb') as file:\n",
    "    pickle.dump(random_df, file)\n",
    "\n",
    "print(f\"Processed text saved to {pickle_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
