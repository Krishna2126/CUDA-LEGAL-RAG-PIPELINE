{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "import pickle\n",
    "from question_answer_functions import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Show full text in columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step to open and load the pickle files\n",
    "pickle_file_path = 'random_question_dataframe.pkl'\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    random_question_dataframe = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_question_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have a function `model_function` that takes a question and returns a predicted answer.\n",
    "def model_function(question,document):\n",
    "    # Placeholder for model's answer generation logic\n",
    "    # Replace with your actual question-answering model function\n",
    "    best_chunk,word_count=user_query(question,document)\n",
    "    # print(best_score)\n",
    "        # Generate outputs using multiple models\n",
    "    \n",
    "    model_outputs_1 = models_output1(question,best_chunk)\n",
    "    model_outputs_2 = models_output2(question,best_chunk)\n",
    "    output = {}\n",
    "    output['gpt-neo-1.3B'] = model_outputs_2\n",
    "    output['roberta-base-cuad-finetuned'] = model_outputs_1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute ROUGE score\n",
    "def compute_rouge(predicted_answer, actual_answer):\n",
    "    scores = scorer.score(predicted_answer, actual_answer)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "predicted_answers = []\n",
    "rouge_scores_list_bert = []\n",
    "rouge_scores_list_llm = []\n",
    "\n",
    "# Loop through each question and get the model's predicted answer\n",
    "for index, row in random_question_dataframe.iterrows():\n",
    "    question = row['question_text']\n",
    "    actual_answer = row['answer_text']\n",
    "    document = row['document_title'] + '.PDF'\n",
    "    \n",
    "    # Get the predicted answer from the model\n",
    "    predicted_answer = model_function(question, document)\n",
    "    print(predicted_answer)\n",
    "    #predicted_answer = ''\n",
    "    \n",
    "    # Compute the ROUGE score\n",
    "    rouge_scores_1 = compute_rouge(predicted_answer['roberta-base-cuad-finetuned'], actual_answer)\n",
    "    rouge_scores_2 = compute_rouge(predicted_answer['gpt-neo-1.3B'], actual_answer)\n",
    "    \n",
    "    # rouge_score_dict = {}\n",
    "    # rouge_score_dict['gpt-neo-1.3B'] = rouge_scores_2\n",
    "    # rouge_score_dict['roberta-base-cuad-finetuned'] = rouge_scores_1\n",
    "    # Store the predicted answer and the ROUGE score\n",
    "    rouge_scores_list_bert.append(rouge_scores_1)\n",
    "    rouge_scores_list_llm.append(rouge_scores_2)\n",
    "\n",
    "    predicted_answers.append(predicted_answer)\n",
    "    # rouge_scores_list.append(rouge_score_dict)\n",
    "\n",
    "# Add predicted answers and ROUGE scores to the DataFrame\n",
    "random_question_dataframe['predicted_answer'] = predicted_answers\n",
    "random_question_dataframe['rouge_score_bert_model'] = rouge_scores_list_bert\n",
    "random_question_dataframe['rouge_score_llm_model'] = rouge_scores_list_llm\n",
    "\n",
    "\n",
    "# Save the DataFrame to CSV for evaluation\n",
    "random_question_dataframe.to_csv('model_predictions_rouge_evaluation.csv', index=False)\n",
    "\n",
    "print(\"Model predictions and ROUGE scores saved to 'model_predictions_rouge_evaluation.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_question_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize lists to store precision, recall, and F1 for each ROUGE metric\n",
    "rouge1_p_bert, rouge1_r_bert, rouge1_f_bert = [], [], []\n",
    "rouge2_p_bert, rouge2_r_bert, rouge2_f_bert = [], [], []\n",
    "rougeL_p_bert, rougeL_r_bert, rougeL_f_bert = [], [], []\n",
    "\n",
    "# Loop through each row in the 'rouge_scores' column\n",
    "for index, row in random_question_dataframe.iterrows():\n",
    "    score = row['rouge_score_bert_model']  # Extract the dictionary for each row\n",
    "    \n",
    "    # Append Rouge-1 scores\n",
    "    rouge1_p_bert.append(score['rouge1'][0])\n",
    "    rouge1_r_bert.append(score['rouge1'][1])\n",
    "    rouge1_f_bert.append(score['rouge1'][2])\n",
    "    \n",
    "    # Append Rouge-2 scores\n",
    "    rouge2_p_bert.append(score['rouge2'][0])\n",
    "    rouge2_r_bert.append(score['rouge2'][1])\n",
    "    rouge2_f_bert.append(score['rouge2'][2])\n",
    "    \n",
    "    # Append Rouge-L scores\n",
    "    rougeL_p_bert.append(score['rougeL'][0])\n",
    "    rougeL_r_bert.append(score['rougeL'][1])\n",
    "    rougeL_f_bert.append(score['rougeL'][2])\n",
    "\n",
    "# Calculate average precision, recall, and F1 for ROUGE-1, ROUGE-2, and ROUGE-L\n",
    "rouge1_avg = (np.mean(rouge1_p_bert), np.mean(rouge1_r_bert), np.mean(rouge1_f_bert))\n",
    "rouge2_avg = (np.mean(rouge2_p_bert), np.mean(rouge2_r_bert), np.mean(rouge2_f_bert))\n",
    "rougeL_avg = (np.mean(rougeL_p_bert), np.mean(rougeL_r_bert), np.mean(rougeL_f_bert))\n",
    "\n",
    "# Print average ROUGE scores\n",
    "print(f\"For roberta-base-cuad-finetuned model ROUGE-1 avg: Precision: {rouge1_avg[0]:.2f}, Recall: {rouge1_avg[1]:.2f}, F1: {rouge1_avg[2]:.2f}\")\n",
    "print(f\"For roberta-base-cuad-finetuned model ROUGE-2 avg: Precision: {rouge2_avg[0]:.2f}, Recall: {rouge2_avg[1]:.2f}, F1: {rouge2_avg[2]:.2f}\")\n",
    "print(f\"For roberta-base-cuad-finetuned model ROUGE-L avg: Precision: {rougeL_avg[0]:.2f}, Recall: {rougeL_avg[1]:.2f}, F1: {rougeL_avg[2]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize lists to store precision, recall, and F1 for each ROUGE metric\n",
    "rouge1_p_llm, rouge1_r_llm, rouge1_f_llm = [], [], []\n",
    "rouge2_p_llm, rouge2_r_llm, rouge2_f_llm = [], [], []\n",
    "rougeL_p_llm, rougeL_r_llm, rougeL_f_llm = [], [], []\n",
    "\n",
    "# Loop through each row in the 'rouge_scores' column\n",
    "for index, row in random_question_dataframe.iterrows():\n",
    "    score = row['rouge_score_llm_model']  # Extract the dictionary for each row\n",
    "    \n",
    "    # Append Rouge-1 scores\n",
    "    rouge1_p_llm.append(score['rouge1'][0])\n",
    "    rouge1_r_llm.append(score['rouge1'][1])\n",
    "    rouge1_f_llm.append(score['rouge1'][2])\n",
    "    \n",
    "    # Append Rouge-2 scores\n",
    "    rouge2_p_llm.append(score['rouge2'][0])\n",
    "    rouge2_r_llm.append(score['rouge2'][1])\n",
    "    rouge2_f_llm.append(score['rouge2'][2])\n",
    "    \n",
    "    # Append Rouge-L scores\n",
    "    rougeL_p_llm.append(score['rougeL'][0])\n",
    "    rougeL_r_llm.append(score['rougeL'][1])\n",
    "    rougeL_f_llm.append(score['rougeL'][2])\n",
    "\n",
    "# Calculate average precision, recall, and F1 for ROUGE-1, ROUGE-2, and ROUGE-L\n",
    "rouge1_avg = (np.mean(rouge1_p_llm), np.mean(rouge1_r_llm), np.mean(rouge1_f_llm))\n",
    "rouge2_avg = (np.mean(rouge2_p_llm), np.mean(rouge2_r_llm), np.mean(rouge2_f_llm))\n",
    "rougeL_avg = (np.mean(rougeL_p_llm), np.mean(rougeL_r_llm), np.mean(rougeL_f_llm))\n",
    "\n",
    "# Print average ROUGE scores\n",
    "print(f\"For gpt-neo-1.3B model ROUGE-1 avg: Precision: {rouge1_avg[0]:.2f}, Recall: {rouge1_avg[1]:.2f}, F1: {rouge1_avg[2]:.2f}\")\n",
    "print(f\"For gpt-neo-1.3B model ROUGE-2 avg: Precision: {rouge2_avg[0]:.2f}, Recall: {rouge2_avg[1]:.2f}, F1: {rouge2_avg[2]:.2f}\")\n",
    "print(f\"For gpt-neo-1.3B model ROUGE-L avg: Precision: {rougeL_avg[0]:.2f}, Recall: {rougeL_avg[1]:.2f}, F1: {rougeL_avg[2]:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
