{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d0167ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f54026b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and load the pickle files\n",
    "pickle_file_path = 'file_contents_pdf.pkl'\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    file_contents_pdf = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "797e9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folder if it doesn't exist\n",
    "output_folder = 'contract_topics'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a40ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d2ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split contract into sentences or chunks\n",
    "def split_contract_into_chunks(text, chunk_size=100):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    chunk_word_count = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words_in_sentence = len(word_tokenize(sentence))\n",
    "        if chunk_word_count + words_in_sentence > chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            chunk_word_count = words_in_sentence\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            chunk_word_count += words_in_sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3613a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess each chunk\n",
    "def preprocess_chunks(chunks):\n",
    "    processed_chunks = []\n",
    "    for chunk in chunks:\n",
    "        processed_chunks.append(preprocess_text(chunk))\n",
    "    return processed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d291eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split and preprocess the contract\n",
    "# chunks = split_contract_into_chunks(single_contract_text)\n",
    "# processed_chunks = preprocess_chunks(chunks)\n",
    "\n",
    "# # Create a dictionary and corpus for LDA\n",
    "# dictionary = corpora.Dictionary(processed_chunks)\n",
    "# corpus = [dictionary.doc2bow(chunk) for chunk in processed_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a7f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute coherence score for different number of topics\n",
    "def compute_coherence_values(dictionary, corpus, texts, start=2, limit=20, step=1):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                id2word=dictionary,\n",
    "                                                num_topics=num_topics, \n",
    "                                                random_state=100,\n",
    "                                                update_every=1,\n",
    "                                                chunksize=100,\n",
    "                                                passes=10,\n",
    "                                                alpha='auto',\n",
    "                                                per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc95545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the optimal number of topics\n",
    "def get_optimal_topics(corpus, dictionary, texts, start=2, limit=10, step=1):\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary, corpus, texts, start=start, limit=limit, step=step)\n",
    "    \n",
    "    # Find the model with the highest coherence score\n",
    "    optimal_index = coherence_values.index(max(coherence_values))\n",
    "    optimal_model = model_list[optimal_index]\n",
    "    optimal_num_topics = range(start, limit, step)[optimal_index]\n",
    "    \n",
    "    print(f\"Optimal number of topics: {optimal_num_topics} with coherence score: {coherence_values[optimal_index]}\")\n",
    "    \n",
    "    return optimal_model, optimal_num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e80afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for 2ThemartComInc_19990826_10-12G_EX-10.10_6700288_EX-10.10_Co-Branding Agreement_ Agency Agreement.pdf already exists. Skipping.\n",
      "CSV for ADAMSGOLFINC_03_21_2005-EX-10.17-ENDORSEMENT AGREEMENT.PDF already exists. Skipping.\n",
      "CSV for ADMA BioManufacturing, LLC -  Amendment #3 to Manufacturing Agreement .PDF already exists. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# Loop over each contract in the dictionary\n",
    "for contract_name, contract_text in file_contents_pdf.items():\n",
    "    # Save the DataFrame to a CSV file with the contract name\n",
    "    output_path = os.path.join(output_folder, f\"{contract_name}.csv\")\n",
    "    \n",
    "    # Check if the CSV already exists; if yes, skip processing\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"CSV for {contract_name} already exists. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing contract: {contract_name}\")\n",
    "    \n",
    "    # Split and preprocess the contract\n",
    "    chunks = split_contract_into_chunks(contract_text)\n",
    "    processed_chunks = preprocess_chunks(chunks)\n",
    "    \n",
    "    # Create dictionary and corpus for the LDA model\n",
    "    dictionary = corpora.Dictionary(processed_chunks)\n",
    "    corpus = [dictionary.doc2bow(chunk) for chunk in processed_chunks]\n",
    "    \n",
    "    # Determine the optimal number of topics\n",
    "    optimal_model, optimal_num_topics = get_optimal_topics(corpus, dictionary, processed_chunks, start=2, limit=20, step=1)\n",
    "    \n",
    "    # Associate each chunk with a dominant topic\n",
    "    chunk_topic_mapping = []\n",
    "    for i, chunk_bow in enumerate(corpus):\n",
    "        topic_distribution = optimal_model.get_document_topics(chunk_bow)\n",
    "        dominant_topic = sorted(topic_distribution, key=lambda x: x[1], reverse=True)[0][0]\n",
    "        chunk_topic_mapping.append((chunks[i], dominant_topic))\n",
    "    \n",
    "    # Group the data by topic and store in a DataFrame\n",
    "    topic_data = {\n",
    "        'Contract_name': [],\n",
    "        'Topic_heading': [],\n",
    "        'Topic_text': []\n",
    "    }\n",
    "\n",
    "    for chunk, topic in chunk_topic_mapping:\n",
    "        topic_data['Contract_name'].append(contract_name)\n",
    "        topic_data['Topic_heading'].append(f'Topic {topic}')\n",
    "        topic_data['Topic_text'].append(chunk)\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(topic_data)\n",
    "\n",
    "    \n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Saved {contract_name}.csv to {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "352843c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the optimal number of topics\n",
    "# optimal_num_topics = x[np.argmax(coherence_values)]\n",
    "# print(f\"Optimal number of topics: {optimal_num_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b133ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the final LDA model with the optimal number of topics\n",
    "# optimal_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "#                                                 id2word=dictionary,\n",
    "#                                                 num_topics=optimal_num_topics, \n",
    "#                                                 random_state=100,\n",
    "#                                                 update_every=1,\n",
    "#                                                 chunksize=100,\n",
    "#                                                 passes=10,\n",
    "#                                                 alpha='auto',\n",
    "#                                                 per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a4b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the topics\n",
    "# topics = optimal_model.print_topics(num_words=20)\n",
    "# for topic in topics:\n",
    "#     print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c90a2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Associate each chunk with a dominant topic\n",
    "# chunk_topic_mapping = []\n",
    "# for i, chunk_bow in enumerate(corpus):\n",
    "#     topic_distribution = optimal_model.get_document_topics(chunk_bow)\n",
    "#     dominant_topic = sorted(topic_distribution, key=lambda x: x[1], reverse=True)[0][0]\n",
    "#     chunk_topic_mapping.append((chunks[i], dominant_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3683cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group and print the chunks under each topic\n",
    "# topic_chunks = defaultdict(list)\n",
    "# for chunk, topic in chunk_topic_mapping:\n",
    "#     topic_chunks[topic].append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb2d7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the text under each topic\n",
    "# for topic, topic_chunk_list in topic_chunks.items():\n",
    "#     print(f\"\\nTopic {topic}:\")\n",
    "#     for idx, chunk in enumerate(topic_chunk_list, 1):\n",
    "#         print(f\"Chunk {idx}: {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "035bfaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group the data by topic and store in a DataFrame\n",
    "# import pandas as pd\n",
    "# topic_data = {\n",
    "#     'Topic_heading': [],\n",
    "#     'Topic_text': []\n",
    "# }\n",
    "\n",
    "# for chunk, topic in chunk_topic_mapping:\n",
    "#     topic_data['Topic_heading'].append(f'Topic {topic}')\n",
    "#     topic_data['Topic_text'].append(chunk)\n",
    "\n",
    "# # Create a pandas DataFrame\n",
    "# df = pd.DataFrame(topic_data)\n",
    "\n",
    "# # Display the DataFrame\n",
    "# print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
